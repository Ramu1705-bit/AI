<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Voice Assistant</title>
    <style>
        body { text-align: center; font-family: Arial, sans-serif; }
        .assistant { font-size: 24px; margin-top: 20px; }
        .face { font-size: 50px; }
        .error { color: red; font-weight: bold; }
    </style>
</head>
<body>
    <h1>AI Voice Assistant</h1>
    <div class="face">üòÉ</div>
    <p class="assistant">Tap the button and speak!</p>
    <p class="error"></p>
    <button id="speakButton">üé§ Speak</button>

    <script>
        const assistantText = document.querySelector(".assistant");
        const face = document.querySelector(".face");
        const errorText = document.querySelector(".error");
        let recognition = null;
        const OLLAMA_API_URL = "http://192.168.1.14:11434/api/generate";

        function isSpeechRecognitionSupported() {
            return 'SpeechRecognition' in window || 'webkitSpeechRecognition' in window;
        }

        function initializeRecognition() {
            if (!isSpeechRecognitionSupported()) {
                errorText.innerText = "Speech recognition is not supported in this browser.";
                return;
            }
            if (!recognition) {
                recognition = new (window.SpeechRecognition || window.webkitSpeechRecognition)();
                recognition.lang = "en-US";
                recognition.interimResults = false;
                recognition.continuous = false;

                recognition.onstart = function() {
                    assistantText.innerText = "Listening...";
                    face.innerText = "üßê";
                };

                recognition.onresult = function(event) {
                    const speech = event.results[0][0].transcript.toLowerCase();
                    assistantText.innerText = "You said: " + speech;
                    getAIResponse(speech);
                };

                recognition.onspeechend = function() {
                    recognition.stop();
                };

                recognition.onerror = function(event) {
                    console.error("Speech Recognition Error: ", event.error);
                    errorText.innerText = "Speech Recognition Error: " + event.error;
                    face.innerText = "üòï";
                };
            }
        }

        async function requestMicAccess() {
            if (!recognition) initializeRecognition();
            try {
                await navigator.mediaDevices.getUserMedia({ audio: true });
                recognition.start();
            } catch (error) {
                errorText.innerText = "Microphone access denied. Please allow it in your browser settings.";
                console.error("Microphone Access Error: ", error);
            }
        }

        async function getAIResponse(inputText) {
            try {
                console.log("Sending request to Ollama API...");
                const response = await fetch(OLLAMA_API_URL, {
                    method: "POST",
                    headers: { "Content-Type": "application/json" },
                    body: JSON.stringify({
                        model: "llama2",
                        prompt: inputText,
                        stream: false
                    })
                });

                if (!response.ok) {
                    throw new Error("Failed to fetch response from AI.");
                }

                const data = await response.json();
                const aiReply = data.response || "I'm not sure how to respond to that.";
                speak(aiReply);
                assistantText.innerText = aiReply;
                face.innerText = "ü§ñ";
            } catch (error) {
                console.error("Error fetching AI response: ", error);
                errorText.innerText = "Error fetching AI response: " + error.message;
            }
        }

        function speak(text) {
            if ('speechSynthesis' in window) {
                const utterance = new SpeechSynthesisUtterance(text);
                utterance.onerror = function(event) {
                    console.error("Speech Synthesis Error: ", event.error);
                    errorText.innerText = "Speech Synthesis Error: " + event.error;
                };
                speechSynthesis.speak(utterance);
            } else {
                console.error("Speech Synthesis not supported in this browser.");
                errorText.innerText = "Speech Synthesis is not supported.";
            }
        }

        document.getElementById("speakButton").addEventListener("click", requestMicAccess);
        initializeRecognition();
    </script>
</body>
</html>